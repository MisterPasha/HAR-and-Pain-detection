{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNjte2gydcpclre8P545b/K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#This files's purpose is to evaluate the performance of the models on different test sets that are trained on 9, 6, and 3 sensors. For the Research question discovering the effect of misalignemnt on decreased number of sensors"],"metadata":{"id":"-Q8yCipbFYea"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6xUl3_RTGsL2","executionInfo":{"status":"ok","timestamp":1740694080256,"user_tz":0,"elapsed":33386,"user":{"displayName":"Pavel Jermolajev","userId":"05342435723381561524"}},"outputId":"c3753e81-9b12-4c90-ef70-e0ced5cda8a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.optim import Adam\n","import torch.nn as nn\n","\n","\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# Upload required test set and relevant trained model"],"metadata":{"id":"JNT8_0DkGDfV"}},{"cell_type":"code","source":["from scipy.io import loadmat\n","\n","# Upload test set from mat file\n","\n","dataset = 5  # defines the test set chosen\n","\n","def define_paths(dataset):\n","  matfile_path = \"\"\n","  model_path = \"\"\n","  if dataset == 1:\n","    matfile_path = \"/content/drive/My Drive/PROJECT/REALDISP/REALDISP_SET_1.mat\"\n","    model_path = \"/content/drive/My Drive/PROJECT/Models/CNNLSTM_DATASET_1/DCL-3L_Framewise_54_96.pth\"\n","  elif dataset == 2:\n","    matfile_path = \"/content/drive/My Drive/PROJECT/REALDISP/REALDISP_SET_2.mat\"\n","    model_path = \"/content/drive/My Drive/PROJECT/Models/CNNLSTM_DATASET_2/DCL-3L_Framewise_54_97.pth\"\n","  elif dataset == 3:\n","    matfile_path = \"/content/drive/My Drive/PROJECT/REALDISP/REALDISP_SET_3.mat\"\n","    model_path = \"/content/drive/My Drive/PROJECT/Models/CNNLSTM_DATASET_3/DCL-3L_Framewise_27_97.pth\"\n","  elif dataset == 4:\n","    matfile_path = \"/content/drive/My Drive/PROJECT/REALDISP/REALDISP_SET_4.mat\"\n","    model_path = \"/content/drive/My Drive/PROJECT/Models/CNNLSTM_DATASET_4/DCL-3L_Framewise_27_94.pth\"\n","  elif dataset == 5:\n","    model_path = \"/content/drive/My Drive/PROJECT/Models/CNNLSTM_DATASET_FULL/DCL-3L_Framewise_81_99.pth\"\n","    matfile_path = \"/content/drive/My Drive/PROJECT/REALDISP/REALDISP_SET_FULL.mat\"\n","  return matfile_path, model_path\n","\n","matfile_path, model_path = define_paths(dataset)\n","\n","# Load the .mat file\n","data = loadmat(matfile_path)\n","\n","# Extract testData and testLabels\n","X_test = data['testData']\n","y_test = data['testLabels'].reshape(-1)\n","\n","# Extract trainData\n","X_train = data['trainData']\n","\n","# Check their shapes\n","print(\"testData shape:\", X_test.shape)\n","print(\"testLabels shape:\", y_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Az9jn7KaQdOX","executionInfo":{"status":"ok","timestamp":1740694093121,"user_tz":0,"elapsed":12867,"user":{"displayName":"Pavel Jermolajev","userId":"05342435723381561524"}},"outputId":"f6589de6-9afe-47b9-d407-30fd73994d02"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dict_keys(['__header__', '__version__', '__globals__', 'trainData', 'valData', 'testData', 'trainLabels', 'valLabels', 'testLabels'])\n","testData shape: (123136, 81)\n","testLabels shape: (123136,)\n"]}]},{"cell_type":"markdown","source":["# Introducing and setting up the trained model for classification"],"metadata":{"id":"CnZGjSrwGcI7"}},{"cell_type":"code","source":["class ConvLSTM(nn.Module):\n","    \"\"\"Model for human-activity-recognition.\"\"\"\n","\n","    def __init__(self, input_channel, num_classes, cnn_channel):\n","        super().__init__()\n","        self.n_layers = 2\n","        self.num_classes = num_classes\n","        self.n_hidden = 128\n","\n","        kernal = (5, 1)\n","\n","        self.features = nn.Sequential(\n","            nn.Conv2d(input_channel, cnn_channel, kernel_size=kernal),\n","            nn.GroupNorm(4, cnn_channel),\n","            nn.MaxPool2d((2, 1)),\n","            nn.ReLU(),\n","            nn.Conv2d(cnn_channel, cnn_channel, kernel_size=kernal),\n","            nn.GroupNorm(4, cnn_channel),\n","            nn.MaxPool2d((2, 1)),\n","            nn.ReLU(),\n","            nn.Conv2d(cnn_channel, cnn_channel, kernel_size=kernal),\n","            nn.GroupNorm(4, cnn_channel),\n","            nn.ReLU(),\n","            # nn.AdaptiveMaxPool2d((4, input_channel))\n","        )\n","\n","        self.lstm1 = nn.LSTM(cnn_channel, hidden_size=self.n_hidden, num_layers=self.n_layers)\n","        self.fc = nn.Linear(self.n_hidden, self.num_classes)\n","        self.dropout = nn.Dropout()\n","\n","    def forward(self, x):\n","        x = x.unsqueeze(1)\n","        x = x.permute(0, 3, 2, 1)\n","        x = self.features(x)\n","\n","        x = x.permute(2, 0, 3, 1)\n","        x = x.reshape(x.shape[0], x.shape[1], -1)\n","\n","        # x, _ = self.lstm1(x)\n","        # x = self.dropout(x)\n","        # out = self.fc(x[:, -1])\n","\n","        x = self.dropout(x)\n","        x, _ = self.lstm1(x)\n","        x = x[-1, :, :]\n","        # x = x.view(x.shape[0], -1, 128)\n","        out = self.fc(x)\n","\n","        return out\n","\n","\n","# Create and upload Model\n","\n","print(f\"This is model path:{model_path}\")\n","\n","model = ConvLSTM(X_test.shape[1], num_classes=33, cnn_channel=256)\n","checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n","\n","optimizer = Adam(model.parameters(), lr=0.001)\n","\n","model.load_state_dict(checkpoint['model_state_dict'])\n","\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Vn47Y2HIZW-","executionInfo":{"status":"ok","timestamp":1740564087843,"user_tz":0,"elapsed":180,"user":{"displayName":"Pavel Jermolajev","userId":"05342435723381561524"}},"outputId":"0205e05c-eade-4698-e4be-edba4c8a6b52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["This is model path:/content/drive/My Drive/PROJECT/Models/CNNLSTM_DATASET_FULL/DCL-3L_Framewise_81_99.pth\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-95-e0c36ca5e664>:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n"]}]},{"cell_type":"code","source":["def drop_rows(sensor_set, sensor_indices, labels, misaligning_row):\n","    \"\"\"\n","    Purpose:\n","        Simulates sensor misalignment by selectively dropping rows from specific sensors\n","        in a multi-sensor dataset. This method is used to mimic the effect of slower\n","        sampling rates or dropped frames in certain sensors.\n","\n","    Args:\n","        sensor_set (list of np.ndarray): A list where each element is a sensor's time-series data\n","                                         (e.g., shape [n_samples, n_features]).\n","        sensor_indices (list of int): Indices (1-based) of the sensors to misalign by dropping rows.\n","        labels (list of np.ndarray): A list of label arrays corresponding to each sensor.\n","        misaligning_row (int): Drop every Nth row to simulate misalignment.\n","\n","    Returns:\n","        updated_sensors (list of np.ndarray): Sensors with dropped rows applied to selected indices.\n","        updated_labels (list of np.ndarray): Corresponding labels with rows dropped to match the data.\n","    \"\"\"\n","\n","    updated_sensors = []\n","    updated_labels = []\n","\n","    for i, sensor in enumerate(sensor_set):\n","        if i + 1 in sensor_indices:  # Check if this sensor should be misaligned (1-based indexing)\n","            # Create a boolean mask: True for rows to keep, False for every Nth row\n","            mask = np.ones(sensor.shape[0], dtype=bool)\n","            mask[::misaligning_row] = False  # Drop every `misaligning_row`th row\n","            updated_sensors.append(sensor[mask])  # Apply mask to sensor data\n","            updated_labels.append(labels[i][mask])  # Apply same mask to corresponding labels\n","        else:\n","            # Leave sensors not specified in sensor_indices unchanged\n","            updated_sensors.append(sensor)\n","            updated_labels.append(labels[i])\n","\n","    return updated_sensors, updated_labels\n","\n","def interpolate(sensor_set, sensor_indices, labels, misaligning_row):\n","    \"\"\"\n","    Interpolates missing rows for selected sensors and adds corresponding labels.\n","\n","    Args:\n","        sensor_set (list of np.ndarray): List of sensor data arrays, each of shape (frames, axes).\n","        sensor_indices (list): List of sensor indices that need interpolation.\n","        labels (np.ndarray): 2D array of shape (frames, num_sensors), containing labels for each frame.\n","        misaligning_row (int): The interval at which rows should be interpolated.\n","\n","    Returns:\n","        tuple: (Updated sensor data list, Updated labels array)\n","    \"\"\"\n","\n","    updated_sensors = []\n","    updated_labels = []\n","\n","    for inx, sensor in enumerate(sensor_set):\n","        sensor_labels = labels[inx]  # Get labels for this sensor\n","        if inx + 1 in sensor_indices:\n","            rows, cols = sensor.shape\n","            interpolated_data = []\n","            interpolated_labels = []\n","\n","            for i in range(rows):\n","                interpolated_data.append(sensor[i])\n","                interpolated_labels.append(sensor_labels[i])\n","\n","                # Interpolate every misaligning_row-th frame\n","                if i % misaligning_row == 0 and i + 1 < rows:\n","                    interpolated_row = (sensor[i] + sensor[i + 1]) / 2  # Compute interpolated values\n","                    interpolated_data.append(interpolated_row)\n","                    interpolated_labels.append(sensor_labels[i])  # Copy previous label\n","\n","            updated_sensors.append(np.array(interpolated_data))  # Convert to NumPy array\n","            updated_labels.append(np.array(interpolated_labels))  # Convert labels to NumPy array\n","        else:\n","            updated_sensors.append(sensor)  # Keep unchanged sensors\n","            updated_labels.append(sensor_labels)  # Keep unchanged labels\n","\n","    # Convert updated_labels list into a stacked NumPy array\n","    #updated_labels = np.stack(updated_labels, axis=1)  # Convert back to (frames, num_sensors)\n","\n","    return updated_sensors, updated_labels\n","\n","\n","def misalign_test_set(sensor_set, labels, num_sensors, misaligning_row=1000):\n","    \"\"\"\n","    Applies misalignment operations on a sensor dataset:\n","    - Drops rows from 1/3 of the sensors\n","    - Interpolates missing rows for 1/3 of the sensors\n","    - Keeps 1/3 of the sensors unchanged\n","\n","    Args:\n","        sensor_set (list of np.ndarray): List of sensor arrays (each of shape (frames, axes)).\n","        labels (np.ndarray): Label array (frames, num_sensors), containing labels per sensor.\n","        num_sensors (int): The number of sensors in the test set (must be 9, 6, or 3).\n","        misaligning_row (int): Row misalignment frequency.\n","\n","    Returns:\n","        tuple: (Updated X_test, Updated y_test)\n","    \"\"\"\n","    assert num_sensors in [9, 6, 3], \"Number of sensors must be 9, 6, or 3\"\n","\n","    # Select indices for each operation\n","    all_sensors = list(range(num_sensors))\n","    np.random.shuffle(all_sensors)\n","\n","    num_to_drop = num_sensors // 3\n","    num_to_interpolate = num_sensors // 3\n","    num_to_keep = num_sensors - num_to_drop - num_to_interpolate\n","\n","    sensor_index_for_drop = all_sensors[:num_to_drop]\n","    sensor_index_for_inter = all_sensors[num_to_drop:num_to_drop + num_to_interpolate]\n","    sensor_index_for_keep = all_sensors[num_to_drop + num_to_interpolate:]\n","\n","    print(f\"Drop Sensors idx: {sensor_index_for_drop}\")\n","    print(f\"Interpolate Sensors idx: {sensor_index_for_inter}\")\n","    print(f\"Keep Sensors idx: {sensor_index_for_keep}\")\n","\n","\n","    # Apply row dropping\n","    sensor_set, label_set = drop_rows(sensor_set, sensor_index_for_drop, labels, misaligning_row)\n","\n","    # Apply interpolation\n","    sensor_set, label_set = interpolate(sensor_set, sensor_index_for_inter, label_set, misaligning_row)\n","\n","    # Trim all sensors to the same length\n","    min_row = min(sensor.shape[0] for sensor in sensor_set)\n","    for i in range(len(sensor_set)):\n","        sensor_set[i] = sensor_set[i][:min_row]\n","        label_set[i] = label_set[i][:min_row]\n","\n","    # Majority voting for final labels\n","    stacked_labels = np.stack(label_set, axis=0)\n","    y_test = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=stacked_labels.astype(int))\n","\n","    # Concatenate sensor data into final feature array\n","    X_test = np.concatenate(sensor_set, axis=1)\n","\n","    return X_test, y_test"],"metadata":{"id":"b2ENoAp_ITyG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Chain test data 9 times to make lenght of 6 hours"],"metadata":{"id":"t4VOD5lwGvEn"}},{"cell_type":"code","source":["X_test = np.tile(X_test, (9, 1))\n","y_test = np.tile(y_test, 9)\n","\n","print(X_test.shape)\n","print(y_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hlrka701ad5G","executionInfo":{"status":"ok","timestamp":1740564088101,"user_tz":0,"elapsed":215,"user":{"displayName":"Pavel Jermolajev","userId":"05342435723381561524"}},"outputId":"abd0cc20-7fb5-4adb-8720-59a8cb4b0b3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1108224, 81)\n","(1108224,)\n"]}]},{"cell_type":"code","source":["# Define the number of features per sensor (e.g., 3-axis accelerometer + 3-axis gyro + 3-axis magnetometer)\n","sensor_length = 9\n","\n","# Determine the total number of sensors based on the shape of X_test\n","num_sensors = X_test.shape[1] // sensor_length\n","print(\"Number of Sensors is: \",num_sensors)\n","\n","# Purpose:\n","# Splits the full test dataset (X_test) into a list of individual sensor data arrays.\n","# Each array corresponds to a sensor and contains only its 9-axis data.\n","sensor_set = [X_test[:, i * sensor_length:(i + 1) * sensor_length] for i in range(num_sensors)]\n","\n","# Creates a corresponding label array for each sensor\n","# Each sensor receives a copy of the original labels (y_test)\n","label_set = np.tile(y_test[:, np.newaxis], num_sensors).T\n","\n","# Applies misalignment to the test set using a custom function\n","# This function is expected to return the misaligned X_test and y_test\n","X_test, y_test = misalign_test_set(sensor_set, label_set, num_sensors)\n","\n","# Output the shapes of the final datasets for confirmation\n","print(\"Final X_test shape:\", X_test.shape)\n","print(\"Final y_test shape:\", y_test.shape)"],"metadata":{"id":"fsjuK4JxPpqb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Normalising data"],"metadata":{"id":"IpjWmVvEHBBl"}},{"cell_type":"code","source":["mean_train = np.mean(X_train, axis=0)\n","std_train = np.std(X_train, axis=0)\n","X_test = (X_test - mean_train) / std_train"],"metadata":{"id":"-I0KabnnG-5R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Segmenting data (Windowing)"],"metadata":{"id":"3q7qDe9OHpWT"}},{"cell_type":"code","source":["from numpy.lib.stride_tricks import as_strided as ast\n","from scipy.stats import mode\n","from collections import Counter\n","\n","import numpy as np\n","from numpy.lib.stride_tricks import as_strided as ast\n","# from scipy.stats import mode  # Uncomment if using mode-based labeling\n","\n","def norm_shape(shape):\n","    \"\"\"\n","    Normalizes input shape into a tuple.\n","    If an integer is passed, converts it to a 1-element tuple.\n","    Raises an error if the input is neither an int nor iterable.\n","    \"\"\"\n","    try:\n","        i = int(shape)\n","        return (i,)\n","    except TypeError:\n","        pass\n","    try:\n","        t = tuple(shape)\n","        return t\n","    except TypeError:\n","        pass\n","    raise TypeError('shape must be an int, or a tuple of ints')\n","\n","def sliding_window(a, ws, ss=None, flatten=True):\n","    \"\"\"\n","    Applies a sliding window over a NumPy array.\n","\n","    Args:\n","        a (np.ndarray): Input array.\n","        ws (int or tuple): Window size.\n","        ss (int or tuple): Step size. If None, defaults to window size (no overlap).\n","        flatten (bool): If True, output is reshaped to 2D.\n","\n","    Returns:\n","        np.ndarray: An array of windowed slices from the input.\n","    \"\"\"\n","    if ss is None:\n","        ss = ws\n","    ws = norm_shape(ws)\n","    ss = norm_shape(ss)\n","    shape = np.array(a.shape)\n","\n","    # Ensure compatible dimensions\n","    ls = [len(shape), len(ws), len(ss)]\n","    if len(set(ls)) != 1:\n","        raise ValueError('a.shape, ws and ss must all have the same length. They were %s' % str(ls))\n","\n","    if np.any(ws > shape):\n","        raise ValueError('ws cannot be larger than a in any dimension. a.shape: %s, ws: %s' % (str(a.shape), str(ws)))\n","\n","    # Compute output shape and strides\n","    newshape = norm_shape(((shape - ws) // ss) + 1) + norm_shape(ws)\n","    newstrides = norm_shape(np.array(a.strides) * ss) + a.strides\n","\n","    strided = ast(a, shape=newshape, strides=newstrides)\n","\n","    if not flatten:\n","        return strided\n","\n","    # Reshape into flat list of slices\n","    meat = len(ws) if ws.shape else 0\n","    firstdim = (np.prod(newshape[:-meat]),) if ws.shape else ()\n","    dim = firstdim + (newshape[-meat:])\n","    return strided.reshape(dim)\n","\n","def data_sliding_window(data_x, data_y, ws, ss):\n","    \"\"\"\n","    Applies a sliding window to both input features and labels.\n","\n","    Args:\n","        data_x (np.ndarray): Feature data of shape [n_samples, n_features].\n","        data_y (np.ndarray): Corresponding labels of shape [n_samples].\n","        ws (int): Window size (number of frames per window).\n","        ss (int): Step size (window shift amount).\n","\n","    Returns:\n","        data_x_windowed (np.ndarray): Windowed feature data of shape [n_windows, ws, n_features].\n","        data_y_windowed (np.ndarray): One label per window (default: last label in window).\n","    \"\"\"\n","    single_label_windows = []\n","\n","    # Apply sliding window on features\n","    data_x = sliding_window(data_x, (ws, data_x.shape[1]), (ss, 1))\n","\n","    # Apply sliding window on labels\n","    #data_y = np.asarray([[i[-1]] for i in sliding_window(data_y, ws, ss)])\n","\n","    # for mode-based labeling, use the line below instead\n","    data_y = np.asarray([[mode(i)[0]] for i in sliding_window(data_y, ws, ss)])\n","\n","    return data_x.astype(np.float32), data_y.reshape(len(data_y)).astype(np.uint8)\n","\n","# -------------------------\n","# Execution of the windowing on test data\n","\n","window_size = 128       # Size of each window (frames)\n","step_size = 64          # Step size (overlap of 50%)\n","\n","# Apply the sliding window\n","X_test_windows, y_test_windows = data_sliding_window(X_test, y_test, window_size, step_size)\n","\n","# Output shapes for confirmation\n","print(\"X_test_windows shape:\", X_test_windows.shape)  # Expected shape: [n_windows, window_size, n_features]\n","print(\"y_test_windows shape:\", y_test_windows.shape)  # Expected shape: [n_windows]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yaDgUrEfTPdi","executionInfo":{"status":"ok","timestamp":1740564102692,"user_tz":0,"elapsed":14557,"user":{"displayName":"Pavel Jermolajev","userId":"05342435723381561524"}},"outputId":"f23cfb5e-3202-4ff7-cc4b-112ea57f72c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-99-7006695be825>:76: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n","  data_x = sliding_window(data_x, (ws, data_x.shape[1]), (ss, 1))\n","<ipython-input-99-7006695be825>:11: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n","  i = int(shape)\n","<ipython-input-99-7006695be825>:78: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n","  data_y = np.asarray([[mode(i)[0]] for i in sliding_window(data_y, ws, ss)])  # Choose most frequent label\n"]},{"output_type":"stream","name":"stdout","text":["X_test_windows shape: (34631, 64, 81)\n","y_test_windows shape: (34631,)\n"]}]},{"cell_type":"markdown","source":["#Converts preprocessed test data (windowed X_test and y_test) into PyTorch tensors, wraps them into a DataLoader, performs batch-wise prediction using a trained model, and collects the predicted and true labels for evaluation."],"metadata":{"id":"9MdgY5N0H4ol"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader, TensorDataset\n","\n","X_test_torch = torch.tensor(X_test_windows, dtype=torch.float32)\n","y_test_torch = torch.tensor(y_test_windows, dtype=torch.long)\n","\n","# Create a DataLoader\n","test_dataset = TensorDataset(X_test_torch, y_test_torch)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","predictions = []\n","true_labels = []\n","\n","with torch.no_grad():\n","    for inputs, labels in test_loader:  # Retrieve both features and labels\n","        outputs = model(inputs)\n","        _, preds = torch.max(outputs, 1)\n","        predictions.extend(preds.numpy())\n","        true_labels.extend(labels.numpy())\n","\n","predictions = np.array(predictions) + 1\n","\n","predictions_half = predictions[int(len(predictions)/2):]\n","true_labels_half = true_labels[int(len(true_labels)/2):]\n","\n","predictions_quat = predictions[int(len(predictions)*0.75):]\n","true_labels_quat = true_labels[int(len(true_labels)*0.75):]"],"metadata":{"id":"nQvhUp8jVNNw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score\n","\n","def per_class_accuracy(true_labels, predictions):\n","    \"\"\"\n","    Computes accuracy for each individual class.\n","    \"\"\"\n","    unique_classes = np.unique(true_labels)  # Get all unique class labels\n","    class_accuracies = {}\n","\n","    for cls in unique_classes:\n","        # Filter predictions and true labels for the current class\n","        mask = true_labels == cls\n","        class_acc = accuracy_score(true_labels[mask], predictions[mask])\n","        class_accuracies[cls] = class_acc  # Store per-class accuracy\n","\n","    return class_accuracies\n","\n","def evaluate_top_bottom_accuracies(true_labels, predictions, name=\"Predictions\"):\n","    \"\"\"\n","    Evaluates:\n","    - Accuracy for overall data\n","    - Accuracy for top 5 classes (most correctly predicted)\n","    - Accuracy for bottom 5 classes (least correctly predicted)\n","    - Mean F1-score\n","    - F1-score Standard Deviation\n","    \"\"\"\n","\n","    print(f\"\\n=== Evaluating {name} ===\")\n","\n","    # Compute overall accuracy\n","    overall_accuracy = accuracy_score(true_labels, predictions)\n","    print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n","\n","    # Compute per-class accuracy\n","    class_accuracies = per_class_accuracy(true_labels, predictions)\n","\n","    # Sort classes by accuracy\n","    sorted_classes = sorted(class_accuracies.items(), key=lambda x: x[1], reverse=True)\n","\n","    # Extract top 5 and bottom 5 classes\n","    top_5_classes = [cls for cls, _ in sorted_classes[:5]]\n","    bottom_5_classes = [cls for cls, _ in sorted_classes[-5:]]\n","\n","    print(f\"Top 5 Classes: {top_5_classes}\")\n","    print(f\"Bottom 5 Classes: {bottom_5_classes}\")\n","\n","    # Filter predictions for top 5 and bottom 5 classes\n","    top5_mask = np.isin(true_labels, top_5_classes)\n","    bottom5_mask = np.isin(true_labels, bottom_5_classes)\n","\n","    # Compute accuracy for top 5 and bottom 5 activities\n","    accuracy_top5 = accuracy_score(true_labels[top5_mask], predictions[top5_mask])\n","    accuracy_bottom5 = accuracy_score(true_labels[bottom5_mask], predictions[bottom5_mask])\n","\n","    print(f\"Top 5 Accuracy: {accuracy_top5:.4f}\")\n","    print(f\"Bottom 5 Accuracy: {accuracy_bottom5:.4f}\")\n","\n","    # Compute Mean F1-score and F1-score Standard Deviation\n","    f1_scores = f1_score(true_labels, predictions, average=None)  # Get F1-score for each class\n","    mean_f1 = np.mean(f1_scores)  # Compute mean F1-score\n","    std_f1 = np.std(f1_scores)  # Compute standard deviation of F1-scores\n","\n","    print(f\"Mean F1-score: {mean_f1:.4f}\")\n","    print(f\"F1-score Standard Deviation: {std_f1:.4f}\")\n","\n","    return {\n","        \"overall_accuracy\": overall_accuracy,\n","        \"top_5_classes\": top_5_classes,\n","        \"bottom_5_classes\": bottom_5_classes,\n","        \"top_5_accuracy\": accuracy_top5,\n","        \"bottom_5_accuracy\": accuracy_bottom5,\n","        \"mean_f1\": mean_f1,\n","        \"std_f1\": std_f1\n","    }\n","\n","\n","results_half = evaluate_top_bottom_accuracies(\n","    np.array(true_labels), np.array(predictions),\n","    name=\"Non-Augmented Predictions (Full)\"\n",")\n","\n","results_half = evaluate_top_bottom_accuracies(\n","    np.array(true_labels_half), np.array(predictions_half),\n","    name=\"Non-Augmented Predictions (Second Half)\"\n",")\n","\n","results_half = evaluate_top_bottom_accuracies(\n","    np.array(true_labels_quat), np.array(predictions_quat),\n","    name=\"Non-Augmented Predictions (Last Quarter)\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Mx68x5pViXq","executionInfo":{"status":"ok","timestamp":1740564199990,"user_tz":0,"elapsed":113,"user":{"displayName":"Pavel Jermolajev","userId":"05342435723381561524"}},"outputId":"8251826a-8b17-4d8f-c6c8-d392bf820a69"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Evaluating Non-Augmented Predictions (Full) ===\n","Overall Accuracy: 0.8346\n","Top 5 Classes: [27, 31, 33, 26, 29]\n","Bottom 5 Classes: [1, 2, 5, 4, 14]\n","Top 5 Accuracy: 0.9705\n","Bottom 5 Accuracy: 0.6804\n","Mean F1-score: 0.8264\n","F1-score Standard Deviation: 0.0943\n","\n","=== Evaluating Non-Augmented Predictions (Second Half) ===\n","Overall Accuracy: 0.8348\n","Top 5 Classes: [27, 33, 31, 26, 29]\n","Bottom 5 Classes: [3, 2, 5, 4, 14]\n","Top 5 Accuracy: 0.9718\n","Bottom 5 Accuracy: 0.6756\n","Mean F1-score: 0.8264\n","F1-score Standard Deviation: 0.0943\n","\n","=== Evaluating Non-Augmented Predictions (Last Quarter) ===\n","Overall Accuracy: 0.8358\n","Top 5 Classes: [27, 31, 33, 26, 29]\n","Bottom 5 Classes: [1, 2, 5, 4, 14]\n","Top 5 Accuracy: 0.9719\n","Bottom 5 Accuracy: 0.6860\n","Mean F1-score: 0.8281\n","F1-score Standard Deviation: 0.0914\n"]}]}]}