{"cells":[{"cell_type":"markdown","source":["# Purpose of this file is to create a Training, Testing, and validation DataFrame dataset out of 17 REALDISP participants' separate data files"],"metadata":{"id":"sDPDdGcZKtcL"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2882,"status":"ok","timestamp":1740483330633,"user":{"displayName":"Pavel Jermolajev","userId":"05342435723381561524"},"user_tz":0},"id":"JF7hpwM55x0_","outputId":"67cacd71-c34f-41bf-f335-26059c125299"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Import libraries\n","from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","\n","drive.mount('/content/drive')\n"]},{"cell_type":"markdown","source":["# Concateate participants data for train, test, valid sets"],"metadata":{"id":"-LSgf9JsMsYE"}},{"cell_type":"code","source":["# -----------------------------\n","# Purpose:\n","# Reads and splits data from REALDISP subject log files into train, validation, and test sets\n","# based on subject ID. Lines with a label of \"0\" are skipped (considered irrelevant or background).\n","# -----------------------------\n","\n","train_data = []\n","valid_data = []\n","test_data = []\n","\n","# Iterate over 17 subject files\n","for i in range(1, 18):\n","    file_path = f\"/content/drive/My Drive/PROJECT/REALDISP/subject{i}_ideal.log\"\n","\n","    with open(file_path, \"r\") as file:  # Automatically closes file after reading\n","        for line in file:\n","            split_line = line.split()\n","            if split_line[-1] != \"0\":  # Exclude lines labeled as \"0\" (non-activity)\n","                if i < 12:\n","                    train_data.append(split_line)      # Subjects 1–11 → Training set\n","                elif i < 15:\n","                    valid_data.append(split_line)      # Subjects 12–14 → Validation set\n","                else:\n","                    test_data.append(split_line)       # Subjects 15–17 → Test set\n"],"metadata":{"id":"rIgYmUloDhGw"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HRjTfUX690l3","executionInfo":{"status":"ok","timestamp":1740483374835,"user_tz":0,"elapsed":19013,"user":{"displayName":"Pavel Jermolajev","userId":"05342435723381561524"}},"outputId":"646772dc-cd07-4e5e-aa3a-4630755f2e17"},"outputs":[{"output_type":"stream","name":"stdout","text":["(435064, 120)\n","(153507, 120)\n","(123251, 120)\n"]}],"source":["# Reformat them in the DataFrames\n","\n","df_train = pd.DataFrame(train_data)\n","df_train = df_train.astype(float)\n","print(df_train.shape)\n","\n","df_valid = pd.DataFrame(valid_data)\n","df_valid = df_valid.astype(float)\n","print(df_valid.shape)\n","\n","df_test = pd.DataFrame(test_data)\n","df_test = df_test.astype(float)\n","print(df_test.shape)"]},{"cell_type":"markdown","source":["#Preprocess Features"],"metadata":{"id":"y_qfz8NKNH2b"}},{"cell_type":"code","source":["# -----------------------------\n","# Purpose:\n","# This script preprocesses the REALDISP dataset by:\n","# 1. Removing the timestamp and label columns.\n","# 2. Dropping quaternion-related columns from the sensor data,\n","#    which are not used in the HAR model.\n","# -----------------------------\n","\n","# Step 1: Remove timestamp and label columns\n","train_features = df_train.iloc[:, 2:-1]\n","train_labels = df_train.iloc[:, -1]\n","\n","valid_features = df_valid.iloc[:, 2:-1]\n","valid_labels = df_valid.iloc[:, -1]\n","\n","test_features = df_test.iloc[:, 2:-1]\n","test_labels = df_test.iloc[:, -1]\n","\n","# -----------------------------\n","# Step 2: Remove quaternion data\n","# Quaternions consist of 4 columns per sensor and appear every 13 columns\n","# Starting from column index 9 (1-based), remove 4 consecutive columns every 13\n","\n","columns_to_remove = []\n","start = 9        # First quaternion column (0-based index)\n","step = 13        # Distance between quaternion blocks\n","remove_count = 4 # Number of columns per quaternion block\n","\n","# Dynamically build list of columns to remove\n","while start <= train_features.shape[1]:\n","    columns_to_remove.extend(range(start, start + remove_count))\n","    start += step\n","\n","# Ensure all indices are within bounds\n","columns_to_remove = [col for col in columns_to_remove if col < train_features.shape[1]]\n","print(\"Columns to remove (quaternions):\", columns_to_remove)\n","\n","# Drop quaternion columns from each dataset\n","train_features = train_features.drop(columns=train_features.columns[columns_to_remove])\n","valid_features = valid_features.drop(columns=valid_features.columns[columns_to_remove])\n","test_features = test_features.drop(columns=test_features.columns[columns_to_remove])\n","\n","# Print final shapes for verification\n","print(\"Train features shape:\", train_features.shape)\n","print(\"Validation features shape:\", valid_features.shape)\n","print(\"Test features shape:\", test_features.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sPbYuzURMQgU","executionInfo":{"status":"ok","timestamp":1740483375188,"user_tz":0,"elapsed":351,"user":{"displayName":"Pavel Jermolajev","userId":"05342435723381561524"}},"outputId":"e2050d85-9de6-46bb-e061-cc4099aa3e31"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(435064, 81)\n","(153507, 81)\n","(123251, 81)\n"]}]},{"cell_type":"markdown","source":["# Segment and shuffle data"],"metadata":{"id":"P_yAZiTGOZtR"}},{"cell_type":"code","source":["import numpy as np\n","\n","def create_variable_size_windows(features, labels, min_size=96, max_size=128):\n","    \"\"\"\n","    Creates windows of variable lengths (randomly chosen between min_size and max_size),\n","    shuffles them, and reshapes them back to the original format.\n","\n","    Args:\n","        features (pd.DataFrame or np.ndarray): Feature matrix (n_samples, n_features).\n","        labels (pd.DataFrame or np.ndarray): Label array (n_samples,).\n","        min_size (int): Minimum window size (default=64).\n","        max_size (int): Maximum window size (default=128).\n","\n","    Returns:\n","        shuffled_features (np.ndarray): Features reshaped back to (n_samples, n_features).\n","        shuffled_labels (np.ndarray): Labels reshaped back to (n_samples,).\n","    \"\"\"\n","    # Convert DataFrames to NumPy arrays if necessary\n","    if not isinstance(features, np.ndarray):\n","        features = features.to_numpy()\n","    if not isinstance(labels, np.ndarray):\n","        labels = labels.to_numpy()\n","\n","    n_samples, n_features = features.shape\n","    windows = []\n","    label_windows = []\n","\n","    # Step 1: Create variable-size windows\n","    i = 0\n","    while i < n_samples - min_size:\n","        # Randomly choose a window size between min_size and max_size\n","        window_size = np.random.randint(min_size, max_size + 1)\n","        #window_sizes.append(window_size)\n","\n","        # Ensure we do not exceed dataset length\n","        if i + window_size > n_samples:\n","            break\n","\n","        # Extract window and corresponding labels\n","        windows.append(features[i : i + window_size, :])  # Now works correctly\n","        label_windows.append(labels[i : i + window_size])\n","\n","        # Move to the next window\n","        i += window_size\n","\n","    # Convert lists to numpy arrays\n","    windows = np.array(windows, dtype=object)  # Use dtype=object for variable-length windows\n","    label_windows = np.array(label_windows, dtype=object)\n","\n","    # Step 2: Shuffle the windows\n","    shuffled_indices = np.random.permutation(len(windows))\n","    windows = windows[shuffled_indices]\n","    label_windows = label_windows[shuffled_indices]\n","\n","    # Step 3: Flatten the shuffled windows back into original shape\n","    shuffled_features = np.vstack(windows)  # Stack into a 2D array\n","    shuffled_labels = np.concatenate(label_windows)  # Flatten into 1D array\n","\n","    return shuffled_features, shuffled_labels\n","\n","# Example Usage:\n","# Assuming train_features, train_labels, valid_features, valid_labels, test_features, test_labels exist\n","shuffled_train_features, shuffled_train_labels = create_variable_size_windows(train_features, train_labels, min_size=128, max_size=128)\n","shuffled_valid_features, shuffled_valid_labels = create_variable_size_windows(valid_features, valid_labels, min_size=128, max_size=128)\n","shuffled_test_features, shuffled_test_labels = create_variable_size_windows(test_features, test_labels, min_size=128, max_size=128)\n","\n","# Print the shapes to verify\n","print(\"Train shape:\", shuffled_train_features.shape, shuffled_train_labels.shape)\n","print(\"Valid shape:\", shuffled_valid_features.shape, shuffled_valid_labels.shape)\n","print(\"Test shape:\", shuffled_test_features.shape, shuffled_test_labels.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"enL2LRiU7uIx","executionInfo":{"status":"ok","timestamp":1740483377801,"user_tz":0,"elapsed":2611,"user":{"displayName":"Pavel Jermolajev","userId":"05342435723381561524"}},"outputId":"eaf6981e-5b41-4237-c074-5fbbb24a4e0b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train shape: (434944, 81) (434944,)\n","Valid shape: (153472, 81) (153472,)\n","Test shape: (123136, 81) (123136,)\n"]}]},{"cell_type":"code","source":["X_train_normal = shuffled_train_features\n","y_train_normal = shuffled_train_labels\n","X_valid = shuffled_valid_features\n","y_valid = shuffled_valid_labels\n","X_test = shuffled_test_features\n","y_test = shuffled_test_labels\n","\n","print(X_train_normal.shape)\n","print(y_train_normal.shape)\n","print(X_valid.shape)\n","print(y_valid.shape)\n","print(X_test.shape)\n","print(y_test.shape)"],"metadata":{"id":"uPcPJum5Oavu","executionInfo":{"status":"ok","timestamp":1740483377877,"user_tz":0,"elapsed":53,"user":{"displayName":"Pavel Jermolajev","userId":"05342435723381561524"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3f3b035d-84a3-4b3d-cc2e-3dbc35640d36"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(434944, 81)\n","(434944,)\n","(153472, 81)\n","(153472,)\n","(123136, 81)\n","(123136,)\n"]}]},{"cell_type":"markdown","source":["# Augmenting training data"],"metadata":{"id":"Ex-SlBeMPK1b"}},{"cell_type":"code","source":["num_sensors = 9\n","sensor_length = 9  # Each sensor has 9 features\n","\n","sensor_set = [X_train_normal[:, i * sensor_length:(i + 1) * sensor_length] for i in range(num_sensors)]\n","\n","sensor_index_for_light_up = [3, 9]  # Right Calf, Right Upper Arm\n","sensor_index_for_light_down = [7, 8]  # Left Upper Arm, Right Lower Arm\n","sensor_index_for_heavy_up = [7, 8, 2]  # Left Upper Arm, Right Lower Arm, Left Thigh\n","sensor_index_for_heavy_down = [3, 9, 4]  # Right Calf, Right Upper Arm, Right Thigh"],"metadata":{"id":"j07tRGxePPPD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from scipy.signal import resample\n","\n","def resample_signal(data, labels, strategy=\"low\", factor=1.0):\n","    \"\"\"\n","    Resamples a 2D sensor data array [frames, axes] by a given factor\n","    and applies the same transformation to the labels.\n","\n","    Args:\n","        data (numpy.ndarray): Input sensor data of shape [frames, 9].\n","        labels (numpy.ndarray): Corresponding labels of shape [frames].\n","        factor (float): Resampling factor (>1.0 for upsampling, <1.0 for downsampling).\n","\n","    Returns:\n","        tuple: (Resampled sensor data, Resampled labels)\n","    \"\"\"\n","\n","    new_length = int(data.shape[0] * factor)  # Compute new frame count\n","    resampled_data = np.zeros((new_length, data.shape[1]))  # Allocate output array\n","\n","    # Resample each sensor axis independently\n","    for i in range(data.shape[1]):\n","        resampled_data[:, i] = resample(data[:, i], new_length)\n","\n","    labels = np.array(labels)\n","\n","    original_indices = np.linspace(0, data.shape[0] - 1, new_length)\n","    indices_affected = np.round(original_indices).astype(int)\n","\n","    if factor < 1:  # Downsampling: Remove affected labels\n","        labels = np.delete(labels, indices_affected)  # Remove indices\n","    elif factor > 1:  # Upsampling: Copy previous label at affected indices\n","        for idx in indices_affected:\n","            if idx > 0:  # Ensure there's a previous label to copy\n","                labels = np.insert(labels, idx, labels[idx - 1])  # Insert previous label\n","\n","    return resampled_data, labels\n","\n","\n","def frequency_warp(data, warp_factor_range=(0.9, 1.1)):\n","    \"\"\"\n","    Applies frequency warping to a 2D sensor data array [frames, axes].\n","\n","    Args:\n","        data (numpy.ndarray): Input sensor data of shape [frames, 9].\n","        warp_factor_range (tuple): Range from which a random warp factor is selected.\n","\n","    Returns:\n","        numpy.ndarray: Frequency-warped sensor data.\n","    \"\"\"\n","    warped_data = np.zeros_like(data)  # Allocate output array\n","\n","    for i in range(data.shape[1]):  # Process each sensor axis independently\n","        warp_factor = np.random.uniform(*warp_factor_range)  # Randomize warp factor per axis\n","\n","        freq_domain = np.fft.fft(data[:, i])  # Compute FFT\n","        freq_domain = np.fft.fftshift(freq_domain) * warp_factor  # Apply warping\n","        warped_data[:, i] = np.real(np.fft.ifft(np.fft.ifftshift(freq_domain)))  # Inverse FFT\n","\n","    return warped_data\n","\n","\n","def random_dropout(data, labels, dropout_rate=0.05):\n","    \"\"\"\n","    Applies random dropout to a 2D sensor data array [frames, axes] and ensures labels match.\n","\n","    Args:\n","        data (numpy.ndarray): Input sensor data of shape [frames, 9].\n","        labels (numpy.ndarray): Corresponding labels of shape [frames].\n","        dropout_rate (float): Percentage of frames to randomly drop.\n","\n","    Returns:\n","        tuple: (Dropout-affected sensor data, Matching labels)\n","    \"\"\"\n","    mask = np.random.rand(data.shape[0]) > dropout_rate  # Generate mask for keeping frames\n","    return data[mask], labels[mask]  # Apply mask to both data and labels"],"metadata":{"id":"uY6qPzoiQLkb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Apply Augmentation"],"metadata":{"id":"CiUWWcf9TR4P"}},{"cell_type":"code","source":["import numpy as np\n","\n","def augment(sensor_set, labels, idx_up, idx_down, strategy=\"low\"):\n","    \"\"\"\n","    Apply augmentation separately to each sensor while keeping alignment.\n","\n","    Args:\n","        sensor_set (list of np.ndarray): List of sensors, each of shape [frames, axes].\n","        labels (np.ndarray): Corresponding labels of shape [frames].\n","        idx_up (list): Indexes of sensors to be upsampled.\n","        idx_down (list): Indexes of sensors to be downsampled.\n","        strategy (str): \"low\" or \"high\" for light or heavy augmentation.\n","\n","    Returns:\n","        tuple: (Augmented sensor data [sensors, min_frames, axes], Voted labels [min_frames])\n","    \"\"\"\n","    augmented_sensors = []\n","    augmented_labels = []\n","\n","    # Define augmentation parameters based on strategy\n","    if strategy == \"low\":\n","        warp_factor = (0.95, 1.05)\n","        dropout = 0.05\n","    else:\n","        warp_factor = (0.9, 1.1)\n","        dropout = 0.1\n","\n","    # Perform augmentation separately for each sensor\n","    for i, sensor in enumerate(sensor_set):\n","        X_train_aug, y_train_aug = sensor, labels.copy()  # Copy labels to prevent modification\n","\n","        # Apply resampling based on sensor index\n","        if i+1 in idx_up:\n","            X_train_aug, y_train_aug = resample_signal(sensor, labels, strategy, 1.000072 if strategy == \"low\" else 1.000142)\n","        elif i+1 in idx_down:\n","            X_train_aug, y_train_aug = resample_signal(sensor, labels, strategy, 0.999931 if strategy == \"low\" else 0.999862)\n","\n","        # Perform frequency warping and dropout\n","        X_train_aug = frequency_warp(X_train_aug, warp_factor_range=warp_factor)\n","        X_train_aug, y_train_aug = random_dropout(X_train_aug, y_train_aug, dropout_rate=dropout)\n","\n","        augmented_sensors.append(X_train_aug)\n","        augmented_labels.append(y_train_aug)\n","\n","    # Find the minimum number of frames across all augmented sensors\n","    min_length = min(sensor.shape[0] for sensor in augmented_sensors)\n","\n","    # Trim all sensor arrays and labels to the minimum length\n","    trimmed_sensors = [sensor[:min_length] for sensor in augmented_sensors]\n","    trimmed_labels = [labels[:min_length] for labels in augmented_labels]\n","\n","    # Stack sensors while maintaining separate sensor rows\n","    stacked_sensors = np.stack(trimmed_sensors, axis=0)  # Shape: [sensors, min_frames, axes]\n","    stacked_labels = np.stack(trimmed_labels, axis=0)  # Shape: [sensors, min_frames]\n","\n","    # Perform label voting (majority label per frame across sensors)\n","    voted_labels = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=stacked_labels.astype(int))\n","\n","    return stacked_sensors, voted_labels\n","\n"],"metadata":{"id":"mcEVSN85TRJI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["light_augmented_sensors, light_augmented_labels = augment(sensor_set, y_train_normal, sensor_index_for_light_up, sensor_index_for_light_down, strategy = \"low\")"],"metadata":{"id":"m8uSBsBxXdHu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["heqvy_augmented_sensors, heqvy_augmented_labels = augment(sensor_set, y_train_normal, sensor_index_for_heavy_up, sensor_index_for_heavy_down, strategy = \"high\")"],"metadata":{"id":"1gQOJ3Z1p6g3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(light_augmented_sensors.shape)\n","print(light_augmented_labels.shape)\n","print(heqvy_augmented_sensors.shape)\n","print(heqvy_augmented_labels.shape)"],"metadata":{"id":"WZYf1sWdYexM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740483400992,"user_tz":0,"elapsed":11,"user":{"displayName":"Pavel Jermolajev","userId":"05342435723381561524"}},"outputId":"b1b3d641-8643-4c65-f384-f7702e5b1e66"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(9, 391152, 9)\n","(391152,)\n"]}]},{"cell_type":"markdown","source":["Bring data to desired shape (num_of_frames, num_of_sensors * num_of_axis), and concatenate all training datasets into 1\n","(e.g. [normal data - lightly augmented data - heavy augmented data])"],"metadata":{"id":"23sB-Q17HHPj"}},{"cell_type":"code","source":["# Step 1: Swap axes to bring frames to the first dimension\n","light_augmented_sensors = np.transpose(light_augmented_sensors, (1, 0, 2))  # Shape: (num_of_frames, num_of_sensors, num_of_axis)\n","heqvy_augmented_sensors = np.transpose(heqvy_augmented_sensors, (1, 0, 2))\n","\n","# Step 2: Reshape to flatten sensor axes into a single row\n","light_augmented_sensors = light_augmented_sensors.reshape(light_augmented_sensors.shape[0], -1)  # Shape: (num_of_frames, num_of_sensors * num_of_axis)\n","heqvy_augmented_sensors = heqvy_augmented_sensors.reshape(heqvy_augmented_sensors.shape[0], -1)\n","\n","print(light_augmented_sensors.shape)\n","print(heqvy_augmented_sensors.shape)\n"],"metadata":{"id":"Oi0aJMerFOXT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740483401528,"user_tz":0,"elapsed":534,"user":{"displayName":"Pavel Jermolajev","userId":"05342435723381561524"}},"outputId":"08637eb0-d184-4ba0-afdd-bdb592145f5e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(391152, 81)\n"]}]},{"cell_type":"code","source":["X_train = np.concatenate((X_train_normal, light_augmented_sensors, heqvy_augmented_sensors), axis=0)\n","y_train = np.concatenate((y_train_normal, light_augmented_labels, heqvy_augmented_labels), axis=0)\n"],"metadata":{"id":"DYbW9PTZHEaz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Chain 3 copies of normal train data for the experiment comparing non-augmented data and augmented data."],"metadata":{"id":"0g_jETFEaAVz"}},{"cell_type":"code","source":["#X_train = np.tile(X_train_normal, (3, 1))\n","#y_train = np.tile(y_train_normal, (3,))"],"metadata":{"id":"nmyoekK4Z_tr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Save as .mat file"],"metadata":{"id":"XUEnWk2dYr7H"}},{"cell_type":"code","source":["X_train = heqvy_augmented_sensors\n","y_train = heqvy_augmented_labels\n","print(len(X_train))\n","print(len(y_train))\n","print(X_valid.shape)\n","print(y_valid.shape)\n","print(X_test.shape)\n","print(y_test.shape)"],"metadata":{"id":"NcskolCadYMQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740483409041,"user_tz":0,"elapsed":52,"user":{"displayName":"Pavel Jermolajev","userId":"05342435723381561524"}},"outputId":"bdb69f30-8b9e-4535-9564-8ba1fdb1a109"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["391152\n","391152\n","(153472, 81)\n","(153472,)\n","(123136, 81)\n","(123136,)\n"]}]},{"cell_type":"code","source":["from scipy.io import savemat\n","\n","data_dict = {\n","    \"trainData\": X_train.tolist(),\n","    \"valData\": X_valid.tolist(),\n","    \"testData\": X_test.tolist(),\n","    \"trainLabels\": y_train.tolist(),\n","    \"valLabels\": y_valid.tolist(),\n","    \"testLabels\": y_test.tolist(),\n","}\n","\n","savemat(\"REALDISP_AUG_NEW_SPLIT.mat\", data_dict)\n"],"metadata":{"id":"JcIkI7BvjDMD"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNi+ITdNjaual4cXEoMAfl3"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}